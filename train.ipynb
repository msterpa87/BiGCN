{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataloader import FakenNewsNet\n",
    "import tensorflow as tf\n",
    "from model import Net, GNNCL\n",
    "from spektral.data import DisjointLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, optimizer, inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(inputs[:2])  # drop edge features from inputs\n",
    "        loss = loss_fn(target, pred)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(binary_accuracy(target, pred))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(data, train_pct=0.75, train_epochs=5, batch_size=1, seed=None):\n",
    "    \"\"\" returns two DataLoaders, resp. for train and test set \n",
    "        according to the input split percentage \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.default_rng(seed)\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    n = len(data)\n",
    "    train_size = int(n*train_pct)\n",
    "\n",
    "    train_set = data[:train_size]\n",
    "    test_set = data[train_size:]\n",
    "\n",
    "    train_loader = DisjointLoader(train_set, batch_size=batch_size, epochs=train_epochs)\n",
    "    test_loader = DisjointLoader(test_set, batch_size=len(test_set), epochs=1)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_loader, test_loader, num_classes=1):\n",
    "    model = GNNCL(num_classes=num_classes)\n",
    "    loss_fn = BinaryCrossentropy()\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    epoch = step = 0\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for inputs, target in tqdm(train_loader):\n",
    "        step += 1\n",
    "        loss, acc = train_step(model, loss_fn, optimizer, inputs, target)\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)\n",
    "\n",
    "        if step == train_loader.steps_per_epoch:\n",
    "            step = 0\n",
    "            epoch += 1\n",
    "            print(\"Ep. {} - Loss: {:.3f}. Acc: {:.3f}\".format(epoch, np.mean(train_losses), np.mean(train_accuracies)))\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "    \n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for inputs, target in test_loader:\n",
    "        pred = model(inputs[:2], training=False)\n",
    "        print(pred[:10], target[:10])\n",
    "        test_losses.append(loss_fn(target, pred))\n",
    "        test_accuracies.append(tf.reduce_mean(binary_accuracy(target, pred)))\n",
    "        \n",
    "    print(\"Done. Test loss: {:.3f}. Test acc: {:.3f}\".format(np.mean(test_losses), np.mean(test_accuracies)))\n",
    "\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281it [00:27, 10.81it/s]"
     ]
    }
   ],
   "source": [
    "data = FakenNewsNet(\"./dataset/FakeNewsNet/politifact/\")  \n",
    "\n",
    "crossval_results = []\n",
    "n_splits = 1\n",
    "\n",
    "for i in range(n_splits):\n",
    "    print(f\"Experiment {i+1} of {n_splits}\")\n",
    "    train_loader, test_loader = random_split(data, train_epochs=50, batch_size=1)\n",
    "\n",
    "    train_loss, train_acc, test_loss, test_acc = evaluate(train_loader, test_loader)\n",
    "\n",
    "    crossval_results.append((train_loss, train_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a0cdf5044e235b24d1fc042fb09c724f9ff16da5a82a5cb2270a9975034c9c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
