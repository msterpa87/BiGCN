{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataloader import FakenNewsNet, WICO\n",
    "import tensorflow as tf\n",
    "from model import GNNCL\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 of 1\n",
      "Ep. 1 - Loss: 7.307. Acc: 0.523\n",
      "Ep. 2 - Loss: 7.247. Acc: 0.527\n",
      "Ep. 3 - Loss: 7.367. Acc: 0.520\n",
      "Ep. 4 - Loss: 7.247. Acc: 0.527\n",
      "Ep. 5 - Loss: 7.397. Acc: 0.518\n",
      "Ep. 6 - Loss: 7.427. Acc: 0.516\n",
      "Ep. 7 - Loss: 7.247. Acc: 0.527\n",
      "Ep. 8 - Loss: 7.427. Acc: 0.516\n",
      "Ep. 9 - Loss: 7.187. Acc: 0.531\n",
      "Done. Test loss: 7.858. Test acc: 0.487\n"
     ]
    }
   ],
   "source": [
    "crossval_results = []\n",
    "n_splits = 1\n",
    "batch_size = 16\n",
    "\n",
    "for i in range(n_splits):\n",
    "    print(f\"Experiment {i+1} of {n_splits}\")\n",
    "    train_loader, test_loader = random_split(data, train_epochs=10, batch_size=batch_size)\n",
    "\n",
    "    train_loss, train_acc, test_loss, test_acc = evaluate(train_loader, test_loader,\n",
    "                                                 channels=64, batch_size=batch_size,\n",
    "                                                 dataset=\"FakeNewsNet\")\n",
    "\n",
    "    crossval_results.append((train_loss, train_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, optimizer, inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = tf.reshape(model(inputs[:2]), (-1,1))\n",
    "        loss = loss_fn(target, pred)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(binary_accuracy(target, pred))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_loader, test_loader, num_classes=1, lr=0.01, channels=64,\n",
    "             batch_size=32, dataset=\"FakeNewsNet\"):\n",
    "    if dataset == \"WICO\":\n",
    "        num_classes = 3\n",
    "        \n",
    "    model = GNNCL(num_classes=num_classes, channels=channels, batch_size=batch_size)\n",
    "    loss_fn = BinaryCrossentropy()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "    epoch = step = 0\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for inputs, target in train_loader:\n",
    "        if target.shape[0] == batch_size:\n",
    "            step += 1\n",
    "            loss, acc = train_step(model, loss_fn, optimizer, inputs, target)\n",
    "            train_losses.append(loss)\n",
    "            train_accuracies.append(acc)\n",
    "\n",
    "        if step == train_loader.steps_per_epoch:\n",
    "            step = 0\n",
    "            epoch += 1\n",
    "            print(\"Ep. {} - Loss: {:.3f}. Acc: {:.3f}\".format(epoch, np.mean(train_losses), np.mean(train_accuracies)))\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "    \n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for inputs, target in test_loader:\n",
    "        if target.shape[0] == batch_size:\n",
    "            pred = tf.reshape(model(inputs[:2], training=False), (-1,1))\n",
    "            test_losses.append(loss_fn(target, pred))\n",
    "            acc = (pred == target)\n",
    "            test_accuracies.append(acc.numpy().mean())\n",
    "        \n",
    "    print(\"Done. Test loss: {:.3f}. Test acc: {:.3f}\".format(np.mean(test_losses), np.mean(test_accuracies)))\n",
    "\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FakenNewsNet(\"./dataset/FakeNewsNet/politifact/\")\n",
    "#data = WICO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a0cdf5044e235b24d1fc042fb09c724f9ff16da5a82a5cb2270a9975034c9c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
