{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, mkdir\n",
    "from os.path import isfile\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing politifact-real: 409 news found.\n",
      "0/409\n",
      "50/409\n",
      "100/409\n",
      "150/409\n",
      "200/409\n",
      "250/409\n",
      "300/409\n",
      "350/409\n",
      "400/409\n",
      "Processing politifact-fake: 392 news found.\n",
      "0/392\n",
      "50/392\n",
      "100/392\n",
      "150/392\n",
      "200/392\n",
      "250/392\n",
      "300/392\n",
      "350/392\n",
      "Processing gossipcop-real: 15759 news found.\n",
      "0/15759\n",
      "50/15759\n",
      "100/15759\n",
      "150/15759\n",
      "200/15759\n",
      "250/15759\n",
      "300/15759\n",
      "350/15759\n",
      "400/15759\n",
      "450/15759\n",
      "500/15759\n",
      "550/15759\n",
      "600/15759\n",
      "650/15759\n",
      "700/15759\n",
      "750/15759\n",
      "800/15759\n",
      "850/15759\n",
      "900/15759\n",
      "950/15759\n",
      "1000/15759\n",
      "1050/15759\n",
      "1100/15759\n",
      "1150/15759\n",
      "1200/15759\n",
      "1250/15759\n",
      "1300/15759\n",
      "1350/15759\n",
      "1400/15759\n",
      "1450/15759\n",
      "1500/15759\n",
      "1550/15759\n",
      "1600/15759\n",
      "1650/15759\n",
      "1700/15759\n",
      "1750/15759\n",
      "1800/15759\n",
      "1850/15759\n",
      "1900/15759\n",
      "1950/15759\n",
      "2000/15759\n",
      "2050/15759\n",
      "2100/15759\n",
      "2150/15759\n",
      "2200/15759\n",
      "2250/15759\n",
      "2300/15759\n",
      "2350/15759\n",
      "2400/15759\n",
      "2450/15759\n",
      "2500/15759\n",
      "2550/15759\n",
      "2600/15759\n",
      "2650/15759\n",
      "2700/15759\n",
      "2750/15759\n",
      "2800/15759\n",
      "2850/15759\n",
      "2900/15759\n",
      "2950/15759\n",
      "3000/15759\n",
      "3050/15759\n",
      "3100/15759\n",
      "3150/15759\n",
      "3200/15759\n",
      "3250/15759\n",
      "3300/15759\n",
      "3350/15759\n",
      "3400/15759\n",
      "3450/15759\n",
      "3500/15759\n",
      "3550/15759\n",
      "3600/15759\n",
      "3650/15759\n",
      "3700/15759\n",
      "3750/15759\n",
      "3800/15759\n",
      "3850/15759\n",
      "3900/15759\n",
      "3950/15759\n",
      "4000/15759\n",
      "4050/15759\n",
      "4100/15759\n",
      "4150/15759\n",
      "4200/15759\n",
      "4250/15759\n",
      "4300/15759\n",
      "4350/15759\n",
      "4400/15759\n",
      "4450/15759\n",
      "4500/15759\n",
      "4550/15759\n",
      "4600/15759\n",
      "4650/15759\n",
      "4700/15759\n",
      "4750/15759\n",
      "4800/15759\n",
      "4850/15759\n",
      "4900/15759\n",
      "4950/15759\n",
      "5000/15759\n",
      "5050/15759\n",
      "5100/15759\n",
      "5150/15759\n",
      "5200/15759\n",
      "5250/15759\n",
      "5300/15759\n",
      "5350/15759\n",
      "5400/15759\n",
      "5450/15759\n",
      "5500/15759\n",
      "5550/15759\n",
      "5600/15759\n",
      "5650/15759\n",
      "5700/15759\n",
      "5750/15759\n",
      "5800/15759\n",
      "5850/15759\n",
      "5900/15759\n",
      "5950/15759\n",
      "6000/15759\n",
      "6050/15759\n",
      "6100/15759\n",
      "6150/15759\n",
      "6200/15759\n",
      "6250/15759\n",
      "6300/15759\n",
      "6350/15759\n",
      "6400/15759\n",
      "6450/15759\n",
      "6500/15759\n",
      "6550/15759\n",
      "6600/15759\n",
      "6650/15759\n",
      "6700/15759\n",
      "6750/15759\n",
      "6800/15759\n",
      "6850/15759\n",
      "6900/15759\n",
      "6950/15759\n",
      "7000/15759\n",
      "7050/15759\n",
      "7100/15759\n",
      "7150/15759\n",
      "7200/15759\n",
      "7250/15759\n",
      "7300/15759\n",
      "7350/15759\n",
      "7400/15759\n",
      "7450/15759\n",
      "7500/15759\n",
      "7550/15759\n",
      "7600/15759\n",
      "7650/15759\n",
      "7700/15759\n",
      "7750/15759\n",
      "7800/15759\n",
      "7850/15759\n",
      "7900/15759\n",
      "7950/15759\n",
      "8000/15759\n",
      "8050/15759\n",
      "8100/15759\n",
      "8150/15759\n",
      "8200/15759\n",
      "8250/15759\n",
      "8300/15759\n",
      "8350/15759\n",
      "8400/15759\n",
      "8450/15759\n",
      "8500/15759\n",
      "8550/15759\n",
      "8600/15759\n",
      "8650/15759\n",
      "8700/15759\n",
      "8750/15759\n",
      "8800/15759\n",
      "8850/15759\n",
      "8900/15759\n",
      "8950/15759\n",
      "9000/15759\n",
      "9050/15759\n",
      "9100/15759\n",
      "9150/15759\n",
      "9200/15759\n",
      "9250/15759\n",
      "9300/15759\n",
      "9350/15759\n",
      "9400/15759\n",
      "9450/15759\n",
      "9500/15759\n",
      "9550/15759\n",
      "9600/15759\n"
     ]
    }
   ],
   "source": [
    "main_path = \"./dataset/FakeNewsNet/\"\n",
    "CLIP_TWEETS = 100   # build subgraphs of the first tweets\n",
    "\n",
    "for dataset in [\"politifact\", \"gossipcop\"]:\n",
    "    for label in [\"real\", \"fake\"]:\n",
    "        path = f\"{main_path}/{dataset}/{label}\"\n",
    "        tweets_path = f\"{path}/tweets\"\n",
    "        graphs_path = f\"{path}/subgraphs\"\n",
    "        features_path = f\"{path}/features\"\n",
    "\n",
    "        with suppress(FileExistsError):\n",
    "            mkdir(graphs_path)\n",
    "\n",
    "        with suppress(FileExistsError):\n",
    "            mkdir(features_path)\n",
    "\n",
    "        edge_lists = {}\n",
    "        nodes_features = {}\n",
    "\n",
    "        tweets_files = listdir(tweets_path)\n",
    "        total_news = len(tweets_files)\n",
    "\n",
    "        print(f\"Processing {dataset}-{label}: {len(tweets_files)} news found.\")\n",
    "\n",
    "        for i,news_id in enumerate(filter(lambda x: dataset in x, tweets_files)):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"{i}/{total_news}\")\n",
    "            \n",
    "            news_path = f\"{tweets_path}/{news_id}\"\n",
    "            subgraph_pathname = f\"{graphs_path}/{news_id}.txt\"\n",
    "            features_pathname = f\"{features_path}/{news_id}.txt\"\n",
    "\n",
    "            if isfile(subgraph_pathname) and isfile(features_pathname):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                json_files = listdir(news_path)\n",
    "            except NotADirectoryError as e:\n",
    "                continue\n",
    "\n",
    "            edge_lists[news_id] = []\n",
    "            nodes_features[news_id] = {}\n",
    "            node_list = []\n",
    "\n",
    "            for filename in filter(lambda x: \"json\" in x, json_files):\n",
    "                node_path = f\"{news_path}/{filename}\"\n",
    "                node = TwitterNode(node_path)\n",
    "                node_list.append(node)\n",
    "            \n",
    "            if len(node_list) == 0:\n",
    "                continue\n",
    "            \n",
    "            # sort nodes by time and clip\n",
    "            node_list = sorted(node_list, key=lambda x: x.created_at)[:CLIP_TWEETS]\n",
    "\n",
    "            # news node edge to each root tweet\n",
    "            edge_lists[news_id] = [(0, node.user_id) for node in node_list]\n",
    "\n",
    "            # adding extra edges of users who mentions other who tweeted the same news\n",
    "            users_with_tweet = set([x.user_id for x in node_list])\n",
    "\n",
    "            for node in node_list:\n",
    "                user_mentioned = set(node.mentions) & users_with_tweet\n",
    "                edge_lists[news_id].extend([(node.user_id, x) for x in user_mentioned])\n",
    "\n",
    "            # adding extra edges for tweets made within timelimit\n",
    "            reversed_node_list = node_list[::-1]\n",
    "\n",
    "            for i,u in enumerate(reversed_node_list):\n",
    "                for v in reversed_node_list[(i+1):]:\n",
    "                    if tweet_hours_diff(u, v) < MAX_TIME_DIFF:\n",
    "                        edge_lists[news_id].append((v.user_id, u.user_id)) # v tweeted before u within maxtime\n",
    "\n",
    "            min_time = min([x.created_at for x in node_list])\n",
    "\n",
    "            # update tweet timestamp to seconds since first tweet of the news\n",
    "            for node in node_list:\n",
    "                node.created_at = int((node.created_at - min_time).total_seconds())\n",
    "                nodes_features[news_id][node.user_id] = node.get_features_vector()\n",
    "            \n",
    "            if len(edge_lists[news_id]) >= MIN_SUBGRAPH_EDGES:\n",
    "                save_edge_list(edge_lists[news_id], subgraph_pathname)\n",
    "                save_node_features(nodes_features[news_id], features_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
