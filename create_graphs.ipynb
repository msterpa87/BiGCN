{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile\n",
    "import networkx as nx\n",
    "from contextlib import suppress\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "MAX_TIME_DIFF = 1\n",
    "MIN_SUBGRAPH_EDGES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED FOR NOW\n",
    "additional_edges = []\n",
    "n = len(node_list)\n",
    "count = 0\n",
    "\n",
    "for i,node1 in enumerate(node_list):\n",
    "    for node2 in node_list[(i+1):]:\n",
    "        diff = tweet_timedelta(node1, node2)\n",
    "\n",
    "        if abs(diff) < MAX_TIME_DIFF:\n",
    "            if diff < 0:\n",
    "                additional_edges.append((node1.user_id, node2.user_id))\n",
    "            else:\n",
    "                additional_edges.append((node2.user_id, node1.user_id))\n",
    "            count += 1\n",
    "\n",
    "print(f\"added {count} of {n*(n-1)/2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_time(timestamp):\n",
    "    format = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "    return datetime.strptime(timestamp, format).replace(tzinfo=None)\n",
    "\n",
    "def months_from_creation(date):\n",
    "    twitter_creation = datetime.strptime(\"Mar 1 2006\", \"%b %d %Y\")\n",
    "    return (date.year - twitter_creation.year)*12 + date.month - twitter_creation.month\n",
    "\n",
    "def save_edge_list(edge_list, pathname):\n",
    "    with open(pathname, \"w\") as f:\n",
    "        for (u,v) in edge_list:\n",
    "            f.write(f\"{u}, {v}\\n\")\n",
    "\n",
    "def save_node_features(node_features, pathname):\n",
    "    with open(pathname, \"w\") as f:\n",
    "        for user_id, vector in node_features.items():\n",
    "            feature_str = \", \".join(list(map(str, vector)))\n",
    "            f.write(f\"{user_id}, {feature_str}\\n\")\n",
    "\n",
    "def tweet_hours_diff(x, y):\n",
    "    return round((x.created_at - y.created_at).total_seconds() / 3600, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNode(object):\n",
    "    def __init__(self, pathname) -> None:\n",
    "        with open(pathname) as f:\n",
    "            node = json.load(f)\n",
    "        \n",
    "        user = node['user']\n",
    "        self.followers_count = user['followers_count']\n",
    "        self.friends_count = user['friends_count']\n",
    "        self.statuses_count = user['statuses_count']\n",
    "        self.favourites_count = user['favourites_count']\n",
    "        self.lists_count = user['listed_count']\n",
    "        self.verified = int(user['verified'])\n",
    "        self.user_created_at = months_from_creation(str_to_time(user['created_at']))\n",
    "        self.user_id = user['id']\n",
    "        self.tweet_id = node['id']\n",
    "        self.mentions = [x['id'] for x in node['entities']['user_mentions']]\n",
    "        self.created_at = str_to_time(node['created_at'])\n",
    "    \n",
    "    def get_features_vector(self):\n",
    "        return [self.verified, self.user_created_at, self.followers_count,\n",
    "                self.friends_count, self.lists_count, self.favourites_count,\n",
    "                self.statuses_count, self.created_at]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing politifact-real: 409 news found.\n",
      "0/411\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Impossibile trovare il percorso specificato: './dataset/FakeNewsNet//politifact/real/politifact100'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-6021717b6c75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mjson_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mNotADirectoryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: './dataset/FakeNewsNet//politifact/real/politifact100'"
     ]
    }
   ],
   "source": [
    "main_path = \"./dataset/FakeNewsNet/\"\n",
    "\n",
    "for dataset in [\"politifact\", \"gossipcop\"]:\n",
    "    for label in [\"real\", \"fake\"]:\n",
    "        path = f\"{main_path}/{dataset}/{label}\"\n",
    "        tweets_path = f\"{path}/tweets\"\n",
    "        graphs_path = f\"{path}/subgraphs\"\n",
    "        features_path = f\"{path}/features\"\n",
    "\n",
    "        with suppress(FileExistsError):\n",
    "            mkdir(graphs_path)\n",
    "\n",
    "        with suppress(FileExistsError):\n",
    "            mkdir(features_path)\n",
    "\n",
    "        edge_lists = {}\n",
    "        nodes_features = {}\n",
    "\n",
    "        tweets_files = listdir(tweets_path)\n",
    "        total_news = len(news_files)\n",
    "\n",
    "        print(f\"Processing {dataset}-{label}: {len(tweets_files)} news found.\")\n",
    "\n",
    "        for i,news_id in enumerate(filter(lambda x: dataset in x, tweets_files)):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"{i}/{total_news}\")\n",
    "            \n",
    "            news_path = f\"{tweets_path}/{news_id}\"\n",
    "            subgraph_pathname = f\"{graphs_path}/{news_id}.txt\"\n",
    "            features_pathname = f\"{features_path}/{news_id}.txt\"\n",
    "\n",
    "            if isfile(subgraph_pathname) and isfile(features_pathname):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                json_files = listdir(news_path)\n",
    "            except NotADirectoryError as e:\n",
    "                continue\n",
    "\n",
    "            edge_lists[news_id] = []\n",
    "            nodes_features[news_id] = {}\n",
    "            node_list = []\n",
    "\n",
    "            for filename in filter(lambda x: \"json\" in x, json_files):\n",
    "                node_path = f\"{news_path}/{filename}\"\n",
    "                node = TwitterNode(node_path)\n",
    "                node_list.append(node)\n",
    "\n",
    "                # news node edge to each root tweet\n",
    "                edge_lists[news_id].append((0, node.user_id))\n",
    "\n",
    "            # adding extra edges for mentions of users in the subgraph\n",
    "            users_with_tweet = set([x.user_id for x in node_list])\n",
    "\n",
    "            for node in node_list:\n",
    "                user_mentioned = set(node.mentions) & users_with_tweet\n",
    "                edge_lists[news_id].extend([(node.user_id, x) for x in user_mentioned])\n",
    "\n",
    "            if len(node_list) == 0:\n",
    "                continue\n",
    "\n",
    "            min_time = min([x.created_at for x in node_list])\n",
    "\n",
    "            # update tweet timestamp to seconds since first tweet of the news\n",
    "            for node in node_list:\n",
    "                node.created_at = int((node.created_at - min_time).total_seconds())\n",
    "                nodes_features[news_id][node.user_id] = node.get_features_vector()\n",
    "            \n",
    "            if len(edge_lists[news_id]) >= MIN_SUBGRAPH_EDGES:\n",
    "                save_edge_list(edge_lists[news_id], subgraph_pathname)\n",
    "                save_node_features(nodes_features[news_id], features_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
